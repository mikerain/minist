{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d68b341-4f62-4539-81a4-b99cb9570e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import os\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6644180f-4233-41f3-bfcd-f20c68531ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 定义网络\n",
    "class C_CNN_Net(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(C_CNN_Net, self).__init__()\n",
    "        self.conv1 = torch.nn.Sequential(torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1),\n",
    "                                         torch.nn.ReLU(),\n",
    "                                         torch.nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
    "                                         torch.nn.ReLU(),\n",
    "                                         torch.nn.MaxPool2d(stride=2, kernel_size=2))\n",
    "        self.dense = torch.nn.Sequential(torch.nn.Linear(14 * 14 * 128, 1024),\n",
    "                                         torch.nn.ReLU(),\n",
    "                                         torch.nn.Dropout(p=0.5),\n",
    "                                         torch.nn.Linear(1024, 10))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = x.view(-1, 14 * 14 * 128)    # 将输出的特征图展开成一维向量\n",
    "        x = self.dense(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def train(epoch):  # 定义每个epoch的训练细节\n",
    "    running_loss = 0\n",
    "    running_correct = 0\n",
    "    train_loss = 0\n",
    "    model.train()  # 设置为trainning模式\n",
    "    batch_num = 0\n",
    "    for epoch in range(1, epoch + 1):  # 以epoch为单位进行循环\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            batch_num += 1\n",
    "            if torch.cuda.is_available():\n",
    "                data, target = data.cuda(), target.cuda()    # 数据迁移到GPU上\n",
    "            optimizer.zero_grad()  # 优化器梯度初始化为零,不然会累加之前的梯度\n",
    "\n",
    "            output = model(data)  # 把数据输入网络并得到输出，即进行前向传播\n",
    "            loss = cost(output, target)  # 计算损失函数\n",
    "            train_loss += loss.item()\n",
    "            loss.backward()  # 反向传播求出输出到每一个节点的梯度\n",
    "            optimizer.step()  # 根据输出到每一个节点的梯度,优化更新参数\n",
    "            if batch_idx % log_interval == 0:  # 准备打印相关信息，args.log_interval是最开头设置的好了的参数\n",
    "                print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                    epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                           100. * batch_idx / len(train_loader), loss.item()))\n",
    "        print(\"Train Epoch\", epoch, \"loss\", train_loss / batch_num)\n",
    "        train_loss = 0\n",
    "    torch.save(model, 'minist_cnn.pkl')  # 保存整个神经网络的结构和模型参数\n",
    "    torch.save(model.state_dict(), 'minist_cnn_params.pkl')  # 只保存神经网络的模型参数\n",
    "\n",
    "\n",
    "def test():\n",
    "    model = torch.load('minist_cnn.pkl')\n",
    "    model.eval()  # 设置为test模式\n",
    "    test_loss = 0  # 初始化测试损失值为0\n",
    "    correct = 0  # 初始化预测正确的数据个数为0\n",
    "    true_wrong = 0\n",
    "    true_right = 0\n",
    "    false_wrong = 0\n",
    "    false_right = 0\n",
    "\n",
    "    for data, target in test_loader:\n",
    "        if torch.cuda.is_available():\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "\n",
    "        output = model(data)  # 数据是按照batch的形式喂入的,然后是这里的输出是全连接层的输出结果\n",
    "        test_loss += (\n",
    "            cost(output, target)).item()  # sum up batch 求loss 把所有loss值进行累加.一般分类器会有softmax,但是这里没有是因为集成在这个损失函数之中了\n",
    "        pred = output.data.max(1, keepdim=True)[\n",
    "            1]  # get the index of the max log-probability #输出的结果虽然会过sofamax转成概率值,但是相对打大小是不变的,所以直接输出取最大值对应的索引就是分类结果\n",
    "        correct += pred.eq(target.data.view_as(pred)).cpu().sum()  # 对预测正确的数据个数进行累加\n",
    "        compare_result = pred.eq(target.data.view_as(pred)).cpu()  # 判断输出和目标数据是不是一样,然后将比较结果转到cpu上\n",
    "        # target=target.numpy()\n",
    "        target = target.cpu()\n",
    "        compare_result = np.array(compare_result)\n",
    "        for i in range(len(compare_result)):\n",
    "            if compare_result[i]:\n",
    "                if target[i] == 1:\n",
    "                    true_right += 1\n",
    "                else:\n",
    "                    false_right += 1\n",
    "            else:\n",
    "                if target[i] == 1:\n",
    "                    true_wrong += 1\n",
    "                else:\n",
    "                    false_wrong += 1\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)  # 因为把所有loss值进行过累加，所以最后要除以总得数据长度才得平均loss\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b26f1540-586f-42a8-b9db-80bb37abe0fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.environ['CUDA_VISIBLE_DEVICES']='1'   # 指定用于训练的GPU，否则默认是第0块GPU\n",
    "# 训练参数\n",
    "# Hyper Parameters\n",
    "\n",
    "EPOCH = 5  # 训练的轮数,这里只迭代一五轮\n",
    "LR = 0.001  # 学习率\n",
    "momentum=0.90\n",
    "batch_size = 200  # 每次训练的时候,放入多少张图片或者是样本\n",
    "\n",
    "\n",
    "# 实例化模型并且打印\n",
    "model = C_CNN_Net()\n",
    "# print(model)\n",
    "\n",
    "# 如果有GPU\n",
    "if torch.cuda.is_available():\n",
    "    model.cuda()  # 将所有的模型参数移动到GPU上\n",
    "    print(\"GPU is using\")\n",
    "\n",
    "# 训练和测试\n",
    "cost = torch.nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=LR, momentum=momentum)  # 初始化优化器 model.train()\n",
    "\n",
    "log_interval = 10  # 每10个batch输出一次信息\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4a90cdb3-7938-44a5-b41b-c0c78d5131e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([60000, 28, 28])\n",
      "torch.Size([60000])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 读取数据\n",
    "data_train = datasets.MNIST(root='./data/',  # 数据集的目录\n",
    "                            train=True,  # 用于训练\n",
    "                            transform=transforms.ToTensor(),  # 转换成张量\n",
    "                            download=True)  # 是否从网络上下载,如果自己已经下载好了可以不用\n",
    "\n",
    "data_test = datasets.MNIST(root='./data/',\n",
    "                           train=False,\n",
    "                           transform=transforms.ToTensor())\n",
    "\n",
    "print(data_train.data.size())  # (60000, 28, 28)\n",
    "print(data_train.targets.size())  # (60000)\n",
    "# 加载数据\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=data_train,\n",
    "                                           batch_size=batch_size,\n",
    "                                           shuffle=True)  # 将数据打乱\n",
    "test_loader = torch.utils.data.DataLoader(dataset=data_test,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "74b8bd22-6ed4-4eef-87b0-3a53eaeaf83f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.303244\n",
      "Train Epoch: 1 [2000/60000 (3%)]\tLoss: 2.299268\n",
      "Train Epoch: 1 [4000/60000 (7%)]\tLoss: 2.285393\n",
      "Train Epoch: 1 [6000/60000 (10%)]\tLoss: 2.267443\n",
      "Train Epoch: 1 [8000/60000 (13%)]\tLoss: 2.249420\n",
      "Train Epoch: 1 [10000/60000 (17%)]\tLoss: 2.230343\n",
      "Train Epoch: 1 [12000/60000 (20%)]\tLoss: 2.218368\n",
      "Train Epoch: 1 [14000/60000 (23%)]\tLoss: 2.180415\n",
      "Train Epoch: 1 [16000/60000 (27%)]\tLoss: 2.160448\n",
      "Train Epoch: 1 [18000/60000 (30%)]\tLoss: 2.115135\n",
      "Train Epoch: 1 [20000/60000 (33%)]\tLoss: 2.076671\n",
      "Train Epoch: 1 [22000/60000 (37%)]\tLoss: 2.023484\n",
      "Train Epoch: 1 [24000/60000 (40%)]\tLoss: 1.923023\n",
      "Train Epoch: 1 [26000/60000 (43%)]\tLoss: 1.852490\n",
      "Train Epoch: 1 [28000/60000 (47%)]\tLoss: 1.683599\n",
      "Train Epoch: 1 [30000/60000 (50%)]\tLoss: 1.547878\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 1.389154\n",
      "Train Epoch: 1 [34000/60000 (57%)]\tLoss: 1.194413\n",
      "Train Epoch: 1 [36000/60000 (60%)]\tLoss: 1.030444\n",
      "Train Epoch: 1 [38000/60000 (63%)]\tLoss: 0.881464\n",
      "Train Epoch: 1 [40000/60000 (67%)]\tLoss: 0.718639\n",
      "Train Epoch: 1 [42000/60000 (70%)]\tLoss: 0.715305\n",
      "Train Epoch: 1 [44000/60000 (73%)]\tLoss: 0.669120\n",
      "Train Epoch: 1 [46000/60000 (77%)]\tLoss: 0.671307\n",
      "Train Epoch: 1 [48000/60000 (80%)]\tLoss: 0.559786\n",
      "Train Epoch: 1 [50000/60000 (83%)]\tLoss: 0.618064\n",
      "Train Epoch: 1 [52000/60000 (87%)]\tLoss: 0.446201\n",
      "Train Epoch: 1 [54000/60000 (90%)]\tLoss: 0.501652\n",
      "Train Epoch: 1 [56000/60000 (93%)]\tLoss: 0.558322\n",
      "Train Epoch: 1 [58000/60000 (97%)]\tLoss: 0.398804\n",
      "Train Epoch 1 loss 1.4321426127354304\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.513747\n",
      "Train Epoch: 2 [2000/60000 (3%)]\tLoss: 0.459684\n",
      "Train Epoch: 2 [4000/60000 (7%)]\tLoss: 0.429073\n",
      "Train Epoch: 2 [6000/60000 (10%)]\tLoss: 0.565742\n",
      "Train Epoch: 2 [8000/60000 (13%)]\tLoss: 0.415053\n",
      "Train Epoch: 2 [10000/60000 (17%)]\tLoss: 0.428966\n",
      "Train Epoch: 2 [12000/60000 (20%)]\tLoss: 0.410242\n",
      "Train Epoch: 2 [14000/60000 (23%)]\tLoss: 0.490362\n",
      "Train Epoch: 2 [16000/60000 (27%)]\tLoss: 0.406687\n",
      "Train Epoch: 2 [18000/60000 (30%)]\tLoss: 0.349902\n",
      "Train Epoch: 2 [20000/60000 (33%)]\tLoss: 0.478071\n",
      "Train Epoch: 2 [22000/60000 (37%)]\tLoss: 0.400876\n",
      "Train Epoch: 2 [24000/60000 (40%)]\tLoss: 0.284708\n",
      "Train Epoch: 2 [26000/60000 (43%)]\tLoss: 0.415892\n",
      "Train Epoch: 2 [28000/60000 (47%)]\tLoss: 0.326955\n",
      "Train Epoch: 2 [30000/60000 (50%)]\tLoss: 0.349183\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.335625\n",
      "Train Epoch: 2 [34000/60000 (57%)]\tLoss: 0.309552\n",
      "Train Epoch: 2 [36000/60000 (60%)]\tLoss: 0.290654\n",
      "Train Epoch: 2 [38000/60000 (63%)]\tLoss: 0.382152\n",
      "Train Epoch: 2 [40000/60000 (67%)]\tLoss: 0.330025\n",
      "Train Epoch: 2 [42000/60000 (70%)]\tLoss: 0.382627\n",
      "Train Epoch: 2 [44000/60000 (73%)]\tLoss: 0.372344\n",
      "Train Epoch: 2 [46000/60000 (77%)]\tLoss: 0.259961\n",
      "Train Epoch: 2 [48000/60000 (80%)]\tLoss: 0.328376\n",
      "Train Epoch: 2 [50000/60000 (83%)]\tLoss: 0.317059\n",
      "Train Epoch: 2 [52000/60000 (87%)]\tLoss: 0.345526\n",
      "Train Epoch: 2 [54000/60000 (90%)]\tLoss: 0.420339\n",
      "Train Epoch: 2 [56000/60000 (93%)]\tLoss: 0.323417\n",
      "Train Epoch: 2 [58000/60000 (97%)]\tLoss: 0.332426\n",
      "Train Epoch 2 loss 0.190336498717467\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.343969\n",
      "Train Epoch: 3 [2000/60000 (3%)]\tLoss: 0.314023\n",
      "Train Epoch: 3 [4000/60000 (7%)]\tLoss: 0.361948\n",
      "Train Epoch: 3 [6000/60000 (10%)]\tLoss: 0.334721\n",
      "Train Epoch: 3 [8000/60000 (13%)]\tLoss: 0.258239\n",
      "Train Epoch: 3 [10000/60000 (17%)]\tLoss: 0.410697\n",
      "Train Epoch: 3 [12000/60000 (20%)]\tLoss: 0.225669\n",
      "Train Epoch: 3 [14000/60000 (23%)]\tLoss: 0.348865\n",
      "Train Epoch: 3 [16000/60000 (27%)]\tLoss: 0.209122\n",
      "Train Epoch: 3 [18000/60000 (30%)]\tLoss: 0.281281\n",
      "Train Epoch: 3 [20000/60000 (33%)]\tLoss: 0.282151\n",
      "Train Epoch: 3 [22000/60000 (37%)]\tLoss: 0.374275\n",
      "Train Epoch: 3 [24000/60000 (40%)]\tLoss: 0.365693\n",
      "Train Epoch: 3 [26000/60000 (43%)]\tLoss: 0.258673\n",
      "Train Epoch: 3 [28000/60000 (47%)]\tLoss: 0.332131\n",
      "Train Epoch: 3 [30000/60000 (50%)]\tLoss: 0.292109\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.306713\n",
      "Train Epoch: 3 [34000/60000 (57%)]\tLoss: 0.261668\n",
      "Train Epoch: 3 [36000/60000 (60%)]\tLoss: 0.332181\n",
      "Train Epoch: 3 [38000/60000 (63%)]\tLoss: 0.249258\n",
      "Train Epoch: 3 [40000/60000 (67%)]\tLoss: 0.281167\n",
      "Train Epoch: 3 [42000/60000 (70%)]\tLoss: 0.248570\n",
      "Train Epoch: 3 [44000/60000 (73%)]\tLoss: 0.321919\n",
      "Train Epoch: 3 [46000/60000 (77%)]\tLoss: 0.251569\n",
      "Train Epoch: 3 [48000/60000 (80%)]\tLoss: 0.241352\n",
      "Train Epoch: 3 [50000/60000 (83%)]\tLoss: 0.190635\n",
      "Train Epoch: 3 [52000/60000 (87%)]\tLoss: 0.231516\n",
      "Train Epoch: 3 [54000/60000 (90%)]\tLoss: 0.272851\n",
      "Train Epoch: 3 [56000/60000 (93%)]\tLoss: 0.291029\n",
      "Train Epoch: 3 [58000/60000 (97%)]\tLoss: 0.237862\n",
      "Train Epoch 3 loss 0.09818143450551563\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.283209\n",
      "Train Epoch: 4 [2000/60000 (3%)]\tLoss: 0.315174\n",
      "Train Epoch: 4 [4000/60000 (7%)]\tLoss: 0.269778\n",
      "Train Epoch: 4 [6000/60000 (10%)]\tLoss: 0.281348\n",
      "Train Epoch: 4 [8000/60000 (13%)]\tLoss: 0.202029\n",
      "Train Epoch: 4 [10000/60000 (17%)]\tLoss: 0.239802\n",
      "Train Epoch: 4 [12000/60000 (20%)]\tLoss: 0.326300\n",
      "Train Epoch: 4 [14000/60000 (23%)]\tLoss: 0.284717\n",
      "Train Epoch: 4 [16000/60000 (27%)]\tLoss: 0.295650\n",
      "Train Epoch: 4 [18000/60000 (30%)]\tLoss: 0.173988\n",
      "Train Epoch: 4 [20000/60000 (33%)]\tLoss: 0.228786\n",
      "Train Epoch: 4 [22000/60000 (37%)]\tLoss: 0.246262\n",
      "Train Epoch: 4 [24000/60000 (40%)]\tLoss: 0.297592\n",
      "Train Epoch: 4 [26000/60000 (43%)]\tLoss: 0.281489\n",
      "Train Epoch: 4 [28000/60000 (47%)]\tLoss: 0.238574\n",
      "Train Epoch: 4 [30000/60000 (50%)]\tLoss: 0.222239\n",
      "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 0.272228\n",
      "Train Epoch: 4 [34000/60000 (57%)]\tLoss: 0.220240\n",
      "Train Epoch: 4 [36000/60000 (60%)]\tLoss: 0.240140\n",
      "Train Epoch: 4 [38000/60000 (63%)]\tLoss: 0.227431\n",
      "Train Epoch: 4 [40000/60000 (67%)]\tLoss: 0.208148\n",
      "Train Epoch: 4 [42000/60000 (70%)]\tLoss: 0.257580\n",
      "Train Epoch: 4 [44000/60000 (73%)]\tLoss: 0.161953\n",
      "Train Epoch: 4 [46000/60000 (77%)]\tLoss: 0.218584\n",
      "Train Epoch: 4 [48000/60000 (80%)]\tLoss: 0.261884\n",
      "Train Epoch: 4 [50000/60000 (83%)]\tLoss: 0.165492\n",
      "Train Epoch: 4 [52000/60000 (87%)]\tLoss: 0.194411\n",
      "Train Epoch: 4 [54000/60000 (90%)]\tLoss: 0.167697\n",
      "Train Epoch: 4 [56000/60000 (93%)]\tLoss: 0.159330\n",
      "Train Epoch: 4 [58000/60000 (97%)]\tLoss: 0.189048\n",
      "Train Epoch 4 loss 0.060324015729129314\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.251118\n",
      "Train Epoch: 5 [2000/60000 (3%)]\tLoss: 0.187391\n",
      "Train Epoch: 5 [4000/60000 (7%)]\tLoss: 0.223976\n",
      "Train Epoch: 5 [6000/60000 (10%)]\tLoss: 0.188316\n",
      "Train Epoch: 5 [8000/60000 (13%)]\tLoss: 0.165853\n",
      "Train Epoch: 5 [10000/60000 (17%)]\tLoss: 0.238385\n",
      "Train Epoch: 5 [12000/60000 (20%)]\tLoss: 0.140929\n",
      "Train Epoch: 5 [14000/60000 (23%)]\tLoss: 0.132101\n",
      "Train Epoch: 5 [16000/60000 (27%)]\tLoss: 0.190656\n",
      "Train Epoch: 5 [18000/60000 (30%)]\tLoss: 0.195486\n",
      "Train Epoch: 5 [20000/60000 (33%)]\tLoss: 0.241789\n",
      "Train Epoch: 5 [22000/60000 (37%)]\tLoss: 0.256627\n",
      "Train Epoch: 5 [24000/60000 (40%)]\tLoss: 0.220975\n",
      "Train Epoch: 5 [26000/60000 (43%)]\tLoss: 0.312175\n",
      "Train Epoch: 5 [28000/60000 (47%)]\tLoss: 0.186741\n",
      "Train Epoch: 5 [30000/60000 (50%)]\tLoss: 0.209788\n",
      "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 0.164281\n",
      "Train Epoch: 5 [34000/60000 (57%)]\tLoss: 0.300444\n",
      "Train Epoch: 5 [36000/60000 (60%)]\tLoss: 0.316243\n",
      "Train Epoch: 5 [38000/60000 (63%)]\tLoss: 0.167622\n",
      "Train Epoch: 5 [40000/60000 (67%)]\tLoss: 0.268585\n",
      "Train Epoch: 5 [42000/60000 (70%)]\tLoss: 0.213893\n",
      "Train Epoch: 5 [44000/60000 (73%)]\tLoss: 0.168054\n",
      "Train Epoch: 5 [46000/60000 (77%)]\tLoss: 0.155965\n",
      "Train Epoch: 5 [48000/60000 (80%)]\tLoss: 0.184717\n",
      "Train Epoch: 5 [50000/60000 (83%)]\tLoss: 0.223155\n",
      "Train Epoch: 5 [52000/60000 (87%)]\tLoss: 0.134775\n",
      "Train Epoch: 5 [54000/60000 (90%)]\tLoss: 0.133259\n",
      "Train Epoch: 5 [56000/60000 (93%)]\tLoss: 0.267718\n",
      "Train Epoch: 5 [58000/60000 (97%)]\tLoss: 0.254242\n",
      "Train Epoch 5 loss 0.040955826779206596\n"
     ]
    }
   ],
   "source": [
    "train(EPOCH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "74045eaf-81c9-4c2d-8c91-009643ba000d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_122478/488275935.py:53: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model = torch.load('minist_cnn.pkl')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.0008, Accuracy: 9541/10000 (95%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb8958d-8c06-40bc-a773-d4e3dd13d874",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
